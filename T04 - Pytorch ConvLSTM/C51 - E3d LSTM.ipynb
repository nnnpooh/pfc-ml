{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import copy\n",
    "import operator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E3DLSTMCell(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_size, kernel_size):\n",
    "        super().__init__()\n",
    "\n",
    "        in_channels = input_shape[0]\n",
    "        self._input_shape = input_shape\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "        # memory gates: input, cell(input modulation), forget\n",
    "        self.weight_xi = ConvDeconv3d(in_channels, hidden_size, kernel_size)\n",
    "        self.weight_hi = ConvDeconv3d(hidden_size, hidden_size, kernel_size, bias=False)\n",
    "\n",
    "        self.weight_xg = copy.deepcopy(self.weight_xi)\n",
    "        self.weight_hg = copy.deepcopy(self.weight_hi)\n",
    "\n",
    "        self.weight_xr = copy.deepcopy(self.weight_xi)\n",
    "        self.weight_hr = copy.deepcopy(self.weight_hi)\n",
    "\n",
    "        memory_shape = list(input_shape)\n",
    "        memory_shape[0] = hidden_size\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(memory_shape)\n",
    "\n",
    "        # for spatiotemporal memory\n",
    "        self.weight_xi_prime = copy.deepcopy(self.weight_xi)\n",
    "        self.weight_mi_prime = copy.deepcopy(self.weight_hi)\n",
    "\n",
    "        self.weight_xg_prime = copy.deepcopy(self.weight_xi)\n",
    "        self.weight_mg_prime = copy.deepcopy(self.weight_hi)\n",
    "\n",
    "        self.weight_xf_prime = copy.deepcopy(self.weight_xi)\n",
    "        self.weight_mf_prime = copy.deepcopy(self.weight_hi)\n",
    "\n",
    "        self.weight_xo = copy.deepcopy(self.weight_xi)\n",
    "        self.weight_ho = copy.deepcopy(self.weight_hi)\n",
    "        self.weight_co = copy.deepcopy(self.weight_hi)\n",
    "        self.weight_mo = copy.deepcopy(self.weight_hi)\n",
    "\n",
    "        self.weight_111 = nn.Conv3d(hidden_size + hidden_size, hidden_size, 1)\n",
    "\n",
    "    def self_attention(self, r, c_history):\n",
    "        batch_size = r.size(0)\n",
    "        channels = r.size(1)\n",
    "        r_flatten = r.view(batch_size, -1, channels)\n",
    "        # BxtaoTHWxC\n",
    "        c_history_flatten = c_history.view(batch_size, -1, channels)\n",
    "\n",
    "        # Attention mechanism\n",
    "        # BxTHWxC x BxtaoTHWxC' = B x THW x taoTHW\n",
    "        scores = torch.einsum(\"bxc,byc->bxy\", r_flatten, c_history_flatten)\n",
    "        attention = F.softmax(scores, dim=2)\n",
    "\n",
    "        return torch.einsum(\"bxy,byc->bxc\", attention, c_history_flatten).view(*r.shape)\n",
    "\n",
    "    def self_attention_fast(self, r, c_history):\n",
    "        # Scaled Dot-Product but for tensors\n",
    "        # instead of dot-product we do matrix contraction on twh dimensions\n",
    "        scaling_factor = 1 / (reduce(operator.mul, r.shape[-3:], 1) ** 0.5)\n",
    "        scores = torch.einsum(\"bctwh,lbctwh->bl\", r, c_history) * scaling_factor\n",
    "\n",
    "        attention = F.softmax(scores, dim=0)\n",
    "        return torch.einsum(\"bl,lbctwh->bctwh\", attention, c_history)\n",
    "\n",
    "    def forward(self, x, c_history, m, h):\n",
    "        # Normalized shape for LayerNorm is CxT×H×W\n",
    "        normalized_shape = list(h.shape[-3:])\n",
    "\n",
    "        def LR(input):\n",
    "            return F.layer_norm(input, normalized_shape)\n",
    "\n",
    "        # R is CxT×H×W\n",
    "        r = torch.sigmoid(LR(self.weight_xr(x) + self.weight_hr(h)))\n",
    "        i = torch.sigmoid(LR(self.weight_xi(x) + self.weight_hi(h)))\n",
    "        g = torch.tanh(LR(self.weight_xg(x) + self.weight_hg(h)))\n",
    "\n",
    "        recall = self.self_attention_fast(r, c_history)\n",
    "        # nice_print(**locals())\n",
    "        # mem_report()\n",
    "        # cpu_stats()\n",
    "\n",
    "        c = i * g + self.layer_norm(c_history[-1] + recall)\n",
    "\n",
    "        i_prime = torch.sigmoid(LR(self.weight_xi_prime(x) + self.weight_mi_prime(m)))\n",
    "        g_prime = torch.tanh(LR(self.weight_xg_prime(x) + self.weight_mg_prime(m)))\n",
    "        f_prime = torch.sigmoid(LR(self.weight_xf_prime(x) + self.weight_mf_prime(m)))\n",
    "\n",
    "        m = i_prime * g_prime + f_prime * m\n",
    "        o = torch.sigmoid(\n",
    "            LR(\n",
    "                self.weight_xo(x)\n",
    "                + self.weight_ho(h)\n",
    "                + self.weight_co(c)\n",
    "                + self.weight_mo(m)\n",
    "            )\n",
    "        )\n",
    "        h = o * torch.tanh(self.weight_111(torch.cat([c, m], dim=1)))\n",
    "\n",
    "        # TODO is it correct FIFO?\n",
    "        c_history = torch.cat([c_history[1:], c[None, :]], dim=0)\n",
    "        # nice_print(**locals())\n",
    "\n",
    "        return (c_history, m, h)\n",
    "\n",
    "    def init_hidden(self, batch_size, tau, device=None):\n",
    "        memory_shape = list(self._input_shape)\n",
    "        memory_shape[0] = self._hidden_size\n",
    "        c_history = torch.zeros(tau, batch_size, *memory_shape, device=device)\n",
    "        m = torch.zeros(batch_size, *memory_shape, device=device)\n",
    "        h = torch.zeros(batch_size, *memory_shape, device=device)\n",
    "\n",
    "        return (c_history, m, h)\n",
    "\n",
    "\n",
    "class ConvDeconv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *vargs, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv3d = nn.Conv3d(in_channels, out_channels, *vargs, **kwargs)\n",
    "        # self.conv_transpose3d = nn.ConvTranspose3d(out_channels, out_channels, *vargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print(self.conv3d(input).shape, input.shape)\n",
    "        # return self.conv_transpose3d(self.conv3d(input))\n",
    "        return F.interpolate(self.conv3d(input), size=input.shape[-3:], mode=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E3DLSTM(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_size, num_layers, kernel_size, tau):\n",
    "        super().__init__()\n",
    "\n",
    "        self._tau = tau\n",
    "        self._cells = []\n",
    "\n",
    "        input_shape = list(input_shape)\n",
    "        for i in range(num_layers):\n",
    "            cell = E3DLSTMCell(input_shape, hidden_size, kernel_size)\n",
    "            # NOTE hidden state becomes input to the next cell\n",
    "            input_shape[0] = hidden_size\n",
    "            self._cells.append(cell)\n",
    "            # Hook to register submodule\n",
    "            setattr(self, \"cell{}\".format(i), cell)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # NOTE (seq_len, batch, input_shape)\n",
    "        batch_size = input.size(1)\n",
    "        c_history_states = []\n",
    "        h_states = []\n",
    "        outputs = []\n",
    "\n",
    "        for step, x in enumerate(input):\n",
    "            for cell_idx, cell in enumerate(self._cells):\n",
    "                if step == 0:\n",
    "                    c_history, m, h = self._cells[cell_idx].init_hidden(\n",
    "                        batch_size, self._tau, input.device\n",
    "                    )\n",
    "                    c_history_states.append(c_history)\n",
    "                    h_states.append(h)\n",
    "\n",
    "                # NOTE c_history and h are coming from the previous time stamp, but we iterate over cells\n",
    "                c_history, m, h = cell(\n",
    "                    x, c_history_states[cell_idx], m, h_states[cell_idx]\n",
    "                )\n",
    "                c_history_states[cell_idx] = c_history\n",
    "                h_states[cell_idx] = h\n",
    "                # NOTE hidden state of previous LSTM is passed as input to the next one\n",
    "                x = h\n",
    "\n",
    "            outputs.append(h)\n",
    "\n",
    "        # NOTE Concat along the channels\n",
    "        return torch.cat(outputs, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 11\n",
    "temporal_frames = 3  # This is not seq length\n",
    "seq_length = 10\n",
    "img_channel = 3\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "hidden_size = 64\n",
    "tau = 2\n",
    "num_layers = 3\n",
    "kernel = (2, 5, 5)  # 3DConv\n",
    "input_shape = (img_channel, temporal_frames, img_height, img_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3d = E3DLSTMCell(input_shape=input_shape, hidden_size=hidden_size, kernel_size=kernel)\n",
    "e3dlstm = E3DLSTM(input_shape, hidden_size, num_layers, kernel_size=kernel, tau=tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figuring out the shape of input\n",
    "\n",
    "Answer: batch_size, img_channel, seq_length , img_height, img_width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_shape = tuple(\n",
    "#     [batch_size, *input_shape]\n",
    "# )  # batch_size, img_channel, temporal_frames, img_height, img_width\n",
    "# X = torch.rand(X_shape)\n",
    "# normalized_shape = list(X.shape[-3:])\n",
    "# print(normalized_shape)\n",
    "# print(X.size())\n",
    "# print(e3d.weight_xr)\n",
    "# X = e3d.weight_xr(X)\n",
    "# print(X.size())\n",
    "# X = F.layer_norm(X, normalized_shape)\n",
    "# print(X.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out the shape of hidden parameters\n",
    "\n",
    "- `c_history`: `(tau, batch_size, hidden_size, temporal_frame, img_height, img_width)`\n",
    "- `h`, `m`: `(batch_size, hidden_size, temporal_frame, img_height, img_width)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LR(input):\n",
    "#     return F.layer_norm(input, normalized_shape)\n",
    "\n",
    "\n",
    "# X_shape = (batch_size, img_channel, temporal_frames, img_height, img_width)\n",
    "# h_shape = (batch_size, hidden_size, temporal_frames, img_height, img_width)\n",
    "# c_history_shape = (tau, batch_size, hidden_size, temporal_frames, img_height, img_width)\n",
    "\n",
    "# #\n",
    "# X = torch.rand(X_shape)\n",
    "# h = torch.rand(h_shape)\n",
    "# c_history = torch.rand(c_history_shape)\n",
    "# print(X.size(), h.size(), c_history.size())\n",
    "# #\n",
    "# normalized_shape = list(h.shape[-3:])\n",
    "# print(normalized_shape)\n",
    "# print(X.size())\n",
    "# r = torch.sigmoid(LR(e3d.weight_xr(X) + e3d.weight_hr(h)))\n",
    "# print(r.size())\n",
    "# scaling_factor = 1 / (reduce(operator.mul, r.shape[-3:], 1) ** 0.5)\n",
    "# print(scaling_factor)\n",
    "# scores = torch.einsum(\"bctwh,lbctwh->bl\", r, c_history) * scaling_factor\n",
    "# attention = F.softmax(scores, dim=0)  # Why is dim not equal to 1???\n",
    "# print(scores.size(), attention.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running a foward pass of E3Dcell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_shape = (batch_size, img_channel, temporal_frames, img_height, img_width)\n",
    "# h_shape = (batch_size, hidden_size, temporal_frames, img_height, img_width)\n",
    "# m_shape = h_shape\n",
    "# c_history_shape = (tau, batch_size, hidden_size, temporal_frames, img_height, img_width)\n",
    "\n",
    "# #\n",
    "# X = torch.rand(X_shape)\n",
    "# h = torch.rand(h_shape)\n",
    "# m = torch.rand(m_shape)\n",
    "# c_history = torch.rand(c_history_shape)\n",
    "# print(X.size(), h.size(), m.size(), c_history.size())\n",
    "# #\n",
    "# c_history, m, h = e3d(X, c_history, m, h)\n",
    "# print(h.size(), m.size(), c_history.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 640, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "X_shape = (seq_length, batch_size, img_channel, temporal_frames, img_height, img_width)\n",
    "\n",
    "\n",
    "X = torch.rand(X_shape)\n",
    "\n",
    "outputs = e3dlstm(X)\n",
    "\n",
    "print(outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1b9c7f2f57af8f07e064f2c72ed76cd3499c7b2fb82bb3d901b6e74555bba53"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
