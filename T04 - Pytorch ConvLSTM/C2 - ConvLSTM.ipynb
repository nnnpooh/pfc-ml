{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle, os\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Original ConvLSTM cell as proposed by Shi et al.\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "    ):\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        if activation == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = torch.relu\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels + out_channels,\n",
    "            out_channels=4 * out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # Initialize weights for Hadamard Products\n",
    "        self.W_ci = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "        self.W_co = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "        self.W_cf = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "\n",
    "    def forward(self, X, H_prev, C_prev):\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n",
    "\n",
    "        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev)\n",
    "        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev)\n",
    "\n",
    "        # Current Cell output\n",
    "        C = forget_gate * C_prev + input_gate * self.activation(C_conv)\n",
    "\n",
    "        output_gate = torch.sigmoid(o_conv + self.W_co * C)\n",
    "\n",
    "        # Current Hidden State\n",
    "        H = output_gate * self.activation(C)\n",
    "\n",
    "        return H, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "    ):\n",
    "\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # We will unroll this over time steps\n",
    "        self.convLSTMcell = ConvLSTMCell(\n",
    "            in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # X is a frame sequence (batch_size, num_channels, seq_len, height, width)\n",
    "\n",
    "        # Get the dimensions\n",
    "        batch_size, _, seq_len, height, width = X.size()\n",
    "\n",
    "        # Initialize output\n",
    "        output = torch.zeros(\n",
    "            batch_size, self.out_channels, seq_len, height, width, device=device\n",
    "        )\n",
    "\n",
    "        # Initialize Hidden State\n",
    "        H = torch.rand(batch_size, self.out_channels, height, width, device=device)\n",
    "\n",
    "        # Initialize Cell Input\n",
    "        C = torch.rand(batch_size, self.out_channels, height, width, device=device)\n",
    "\n",
    "        # Unroll over time steps\n",
    "        for time_step in range(seq_len):\n",
    "\n",
    "            H, C = self.convLSTMcell(X[:, :, time_step], H, C)\n",
    "\n",
    "            output[:, :, time_step] = H\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels,\n",
    "        num_kernels,\n",
    "        kernel_size,\n",
    "        padding,\n",
    "        activation,\n",
    "        frame_size,\n",
    "        num_layers,\n",
    "    ):\n",
    "\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.sequential = nn.Sequential()\n",
    "\n",
    "        # Add First layer (Different in_channels than the rest)\n",
    "        self.sequential.add_module(\n",
    "            \"convlstm1\",\n",
    "            ConvLSTM(\n",
    "                in_channels=num_channels,\n",
    "                out_channels=num_kernels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                activation=activation,\n",
    "                frame_size=frame_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.sequential.add_module(\n",
    "            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "        )\n",
    "\n",
    "        # Add rest of the layers\n",
    "        for l in range(2, num_layers + 1):\n",
    "\n",
    "            self.sequential.add_module(\n",
    "                f\"convlstm{l}\",\n",
    "                ConvLSTM(\n",
    "                    in_channels=num_kernels,\n",
    "                    out_channels=num_kernels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    activation=activation,\n",
    "                    frame_size=frame_size,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            self.sequential.add_module(\n",
    "                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "            )\n",
    "\n",
    "        # Add Convolutional Layer to predict output frame\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=num_kernels,\n",
    "            out_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Forward propagation through all the layers\n",
    "        output = self.sequential(X)\n",
    "\n",
    "        # Return only the last output frame\n",
    "        output = self.conv(output[:, :, -1])\n",
    "\n",
    "        return nn.Sigmoid()(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `ConvLSTMCell` and `ConvLSTM` classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "out_channels = 10\n",
    "kernel_size = 3\n",
    "padding = \"same\"\n",
    "activation = \"relu\"\n",
    "frame_size = (5, 5)\n",
    "batch_size = 3\n",
    "seq_length = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1 = ConvLSTMCell(\n",
    "    in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    ")\n",
    "torch.manual_seed(0)\n",
    "X = torch.rand(batch_size, in_channels, seq_length, *frame_size)\n",
    "H = torch.rand(batch_size, out_channels, *frame_size)\n",
    "C = torch.rand(batch_size, out_channels, *frame_size)\n",
    "O1 = torch.zeros(batch_size, out_channels, seq_length, *frame_size)\n",
    "for timestep in range(seq_length):\n",
    "    H, C = lstm1(X[:, :, timestep], H, C)\n",
    "    O1[:, :, timestep] = H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm2 = ConvLSTM(\n",
    "    in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    ")\n",
    "torch.manual_seed(0)\n",
    "X = torch.rand(batch_size, in_channels, seq_length, *frame_size)\n",
    "O2 = lstm2(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1593, 0.0632,    nan, 0.0795, 0.0000], grad_fn=<SelectBackward0>)\n",
      "tensor([0.3792, 0.0239, 0.1403, 0.1350, 0.2896], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This should give the same answer.\n",
    "print(O1[0, 0, 0, 0])\n",
    "print(O2[0, 0, 0, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `Seq2Seq` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 10, 5, 5])\n",
      "torch.Size([3, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "num_channels = in_channels\n",
    "num_kernels = out_channels\n",
    "num_layers = 3\n",
    "\n",
    "lstm3 = Seq2Seq(\n",
    "    num_channels, num_kernels, kernel_size, padding, activation, frame_size, num_layers\n",
    ")\n",
    "\n",
    "X = torch.rand(batch_size, num_channels, seq_length, *frame_size)\n",
    "O3 = lstm3(X)\n",
    "\n",
    "print(X.size())\n",
    "print(O3.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "out_channels = 10\n",
    "kernel_size = (1, 3)\n",
    "padding = \"same\"\n",
    "activation = \"relu\"\n",
    "frame_size = (1, 32)\n",
    "batch_size = 3\n",
    "seq_length = 10\n",
    "num_channels = in_channels\n",
    "num_kernels = out_channels\n",
    "num_layers = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nnnpooh/Coding/research/pfc-ml/T02 - PFC - ConvLSTM/output/2022-05-08-07-21-22/data_store.pickle\n"
     ]
    }
   ],
   "source": [
    "# Data folder\n",
    "cwd = os.getcwd()\n",
    "folderPFC = \"T02 - PFC - ConvLSTM\"\n",
    "folderOutput = \"2022-05-08-07-21-22\"\n",
    "rootFolder = os.path.dirname(cwd)  # Up one level\n",
    "filePath = os.path.join(\n",
    "    rootFolder, folderPFC, \"output\", folderOutput, \"data_store.pickle\"\n",
    ")\n",
    "print(filePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "with open(filePath, \"rb\") as handle:\n",
    "    data_dict = pickle.load(handle)\n",
    "\n",
    "data = data_dict[\"data\"]\n",
    "# nCol = data_dict[\"n\"]\n",
    "# nRow = 1\n",
    "# nChannel = 1\n",
    "# tFinal = data_dict[\"tf\"]\n",
    "# dx = data_dict[\"dx\"]\n",
    "# L = data_dict[\"L\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, seq_length, num_channels, frame_size):\n",
    "        X_, y_ = self.split_sequence(data, seq_length)\n",
    "        X_ = X_.reshape(X_.shape[0], num_channels, seq_length, *frame_size)\n",
    "        self.X = torch.tensor(X_)\n",
    "        self.y = torch.tensor(y_)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def split_sequence(self, data, nSeq):\n",
    "        X, y = list(), list()\n",
    "        nRow = data.shape[0]\n",
    "        for i in range(nRow):\n",
    "            # find the end of this pattern\n",
    "            end_ix = i + nSeq\n",
    "            # check if we are beyond the sequence\n",
    "            if end_ix > nRow - 1:\n",
    "                break\n",
    "                # gather input and output parts of the pattern\n",
    "            seq_x, seq_y = data[i:end_ix, :], data[end_ix, :]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Select sample\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 10, 1, 32])\n",
      "torch.Size([2, 32])\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset(data, seq_length, num_channels, frame_size)\n",
    "train_loader = DataLoader(ds, shuffle=True, batch_size=2)\n",
    "train_ = next(iter(train_loader))\n",
    "print(train_[0].shape)\n",
    "print(train_[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f28be32ba60>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "phi_ = train_[0][0,0,1,0,:].cpu().detach().numpy()\n",
    "plt.plot(phi_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/nnnpooh/Coding/research/pfc-ml/T04 - Pytorch ConvLSTM/C2 - ConvLSTM.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000034?line=0'>1</a>\u001b[0m lstm3 \u001b[39m=\u001b[39m Seq2Seq(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000034?line=1'>2</a>\u001b[0m     num_channels, num_kernels, kernel_size, padding, activation, frame_size, num_layers\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000034?line=2'>3</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000034?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m count, (train_data, train_label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000034?line=6'>7</a>\u001b[0m     lstm3(train_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/nnnpooh/Coding/research/pfc-ml/T04 - Pytorch ConvLSTM/C2 - ConvLSTM.ipynb Cell 4'\u001b[0m in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000002?line=60'>61</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000002?line=61'>62</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000002?line=62'>63</a>\u001b[0m     \u001b[39m# Forward propagation through all the layers\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000002?line=63'>64</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msequential(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000002?line=65'>66</a>\u001b[0m     \u001b[39m# Return only the last output frame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000002?line=66'>67</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(output[:, :, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/nnnpooh/Coding/research/pfc-ml/T04 - Pytorch ConvLSTM/C2 - ConvLSTM.ipynb Cell 3'\u001b[0m in \u001b[0;36mConvLSTM.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000001?line=32'>33</a>\u001b[0m \u001b[39m# Unroll over time steps\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000001?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m time_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(seq_len):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000001?line=35'>36</a>\u001b[0m     H, C \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvLSTMcell(X[:, :, time_step], H, C)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000001?line=37'>38</a>\u001b[0m     output[:, :, time_step] \u001b[39m=\u001b[39m H\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000001?line=39'>40</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/nnnpooh/Coding/research/pfc-ml/T04 - Pytorch ConvLSTM/C2 - ConvLSTM.ipynb Cell 2'\u001b[0m in \u001b[0;36mConvLSTMCell.forward\u001b[0;34m(self, X, H_prev, C_prev)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000000?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X, H_prev, C_prev):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000000?line=29'>30</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000000?line=30'>31</a>\u001b[0m     \u001b[39m# Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000000?line=31'>32</a>\u001b[0m     conv_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(torch\u001b[39m.\u001b[39;49mcat([X, H_prev], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000000?line=33'>34</a>\u001b[0m     \u001b[39m# Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nnnpooh/Coding/research/pfc-ml/T04%20-%20Pytorch%20ConvLSTM/C2%20-%20ConvLSTM.ipynb#ch0000000?line=34'>35</a>\u001b[0m     i_conv, f_conv, C_conv, o_conv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mchunk(conv_output, chunks\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///home/nnnpooh/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "lstm3 = Seq2Seq(\n",
    "    num_channels, num_kernels, kernel_size, padding, activation, frame_size, num_layers\n",
    ")\n",
    "\n",
    "\n",
    "for count, (train_data, train_label) in enumerate(train_loader):\n",
    "    lstm3(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d201a29f7c4182ebe0d045b868be9ef5a41c2ba4cccd19f1728f184a837bb8e6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
