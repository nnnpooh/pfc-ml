{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle, os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Original ConvLSTM cell as proposed by Shi et al.\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "    ):\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        if activation == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = torch.relu\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels + out_channels,\n",
    "            out_channels=4 * out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        # Initialize weights for Hadamard Products\n",
    "        self.W_ci = nn.Parameter(torch.rand(out_channels, *frame_size))\n",
    "        self.W_co = nn.Parameter(torch.rand(out_channels, *frame_size))\n",
    "        self.W_cf = nn.Parameter(torch.rand(out_channels, *frame_size))\n",
    "\n",
    "    def forward(self, X, H_prev, C_prev):\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n",
    "\n",
    "        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev)\n",
    "        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev)\n",
    "\n",
    "        # Current Cell output\n",
    "        C = forget_gate * C_prev + input_gate * self.activation(C_conv)\n",
    "\n",
    "        output_gate = torch.sigmoid(o_conv + self.W_co * C)\n",
    "\n",
    "        # Current Hidden State\n",
    "        H = output_gate * self.activation(C)\n",
    "\n",
    "        return H, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "    ):\n",
    "\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # We will unroll this over time steps\n",
    "        self.convLSTMcell = ConvLSTMCell(\n",
    "            in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # X is a frame sequence (batch_size, num_channels, seq_len, height, width)\n",
    "\n",
    "        # Get the dimensions\n",
    "        batch_size, _, seq_len, height, width = X.size()\n",
    "\n",
    "        # Initialize output\n",
    "        output = torch.zeros(\n",
    "            batch_size, self.out_channels, seq_len, height, width, device=device\n",
    "        )\n",
    "\n",
    "        # Initialize Hidden State\n",
    "        H = torch.zeros(batch_size, self.out_channels, height, width, device=device)\n",
    "\n",
    "        # Initialize Cell Input\n",
    "        C = torch.zeros(batch_size, self.out_channels, height, width, device=device)\n",
    "\n",
    "        # Unroll over time steps\n",
    "        for time_step in range(seq_len):\n",
    "\n",
    "            H, C = self.convLSTMcell(X[:, :, time_step], H, C)\n",
    "\n",
    "            output[:, :, time_step] = H\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels,\n",
    "        num_kernels,\n",
    "        kernel_size,\n",
    "        padding,\n",
    "        activation,\n",
    "        frame_size,\n",
    "        num_layers,\n",
    "    ):\n",
    "\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.sequential = nn.Sequential()\n",
    "\n",
    "        # Add First layer (Different in_channels than the rest)\n",
    "        self.sequential.add_module(\n",
    "            \"convlstm1\",\n",
    "            ConvLSTM(\n",
    "                in_channels=num_channels,\n",
    "                out_channels=num_kernels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                activation=activation,\n",
    "                frame_size=frame_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.sequential.add_module(\n",
    "            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "        )\n",
    "\n",
    "        # Add rest of the layers\n",
    "        for l in range(2, num_layers + 1):\n",
    "\n",
    "            self.sequential.add_module(\n",
    "                f\"convlstm{l}\",\n",
    "                ConvLSTM(\n",
    "                    in_channels=num_kernels,\n",
    "                    out_channels=num_kernels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    activation=activation,\n",
    "                    frame_size=frame_size,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            self.sequential.add_module(\n",
    "                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "            )\n",
    "\n",
    "        # Add Convolutional Layer to predict output frame\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=num_kernels,\n",
    "            out_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Forward propagation through all the layers\n",
    "        output = self.sequential(X)\n",
    "\n",
    "        # Return only the last output frame\n",
    "        output = self.conv(output[:, :, -1])\n",
    "\n",
    "        return nn.Sigmoid()(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `ConvLSTMCell` and `ConvLSTM` classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "out_channels = 10\n",
    "kernel_size = 3\n",
    "padding = \"same\"\n",
    "activation = \"relu\"\n",
    "frame_size = (5, 5)\n",
    "batch_size = 4\n",
    "seq_length = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1 = ConvLSTMCell(\n",
    "    in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    ")\n",
    "torch.manual_seed(1)\n",
    "X = torch.rand(batch_size, in_channels, seq_length, *frame_size)\n",
    "H = torch.zeros(batch_size, out_channels, *frame_size)\n",
    "C = torch.zeros(batch_size, out_channels, *frame_size)\n",
    "O1 = torch.zeros(batch_size, out_channels, seq_length, *frame_size)\n",
    "for timestep in range(seq_length):\n",
    "    H, C = lstm1(X[:, :, timestep], H, C)\n",
    "    O1[:, :, timestep] = H\n",
    "\n",
    "print(X.shape)\n",
    "print(O1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm2 = ConvLSTM(\n",
    "    in_channels, out_channels, kernel_size, padding, activation, frame_size\n",
    ")\n",
    "torch.manual_seed(1)\n",
    "X = torch.rand(batch_size, in_channels, seq_length, *frame_size)\n",
    "O2 = lstm2(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(O2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should give the same answer.\n",
    "print(O1[0, 0, -1, 0])\n",
    "print(O2[0, 0, -1, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `Seq2Seq` class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = in_channels\n",
    "num_kernels = out_channels\n",
    "num_layers = 3\n",
    "\n",
    "lstm3 = Seq2Seq(\n",
    "    num_channels, num_kernels, kernel_size, padding, activation, frame_size, num_layers\n",
    ")\n",
    "\n",
    "X = torch.rand(batch_size, num_channels, seq_length, *frame_size)\n",
    "O3 = lstm3(X)\n",
    "\n",
    "print(X.size())\n",
    "print(O3.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PFC data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Be careful when setting batch_size to 1 because BatchNorm will not work correctly.\n",
    "- https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "out_channels = 10\n",
    "kernel_size = (1, 3)\n",
    "padding = \"same\"\n",
    "activation = \"tanh\"\n",
    "frame_size = (1, 32)\n",
    "batch_size = 10 \n",
    "seq_length = 10\n",
    "num_channels = in_channels\n",
    "num_kernels = out_channels\n",
    "num_layers = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data folder\n",
    "cwd = os.getcwd()\n",
    "folderPFC = \"T02 - PFC - ConvLSTM\"\n",
    "folderOutput = \"2022-05-08-07-21-22\"\n",
    "rootFolder = os.path.dirname(cwd)  # Up one level\n",
    "filePath = os.path.join(\n",
    "    rootFolder, folderPFC, \"output\", folderOutput, \"data_store.pickle\"\n",
    ")\n",
    "print(filePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "with open(filePath, \"rb\") as handle:\n",
    "    data_dict = pickle.load(handle)\n",
    "\n",
    "data = data_dict[\"data\"]\n",
    "# nCol = data_dict[\"n\"]\n",
    "# nRow = 1\n",
    "# nChannel = 1\n",
    "# tFinal = data_dict[\"tf\"]\n",
    "# dx = data_dict[\"dx\"]\n",
    "# L = data_dict[\"L\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to sigmoid function, the output cannot be negative.  I could however, use linear layer to fix this?\n",
    "maxVal = data.max()\n",
    "minVal = data.min()\n",
    "dataScaled = (data - minVal) / (maxVal - minVal)\n",
    "plt.plot(dataScaled[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, seq_length, num_channels, frame_size):\n",
    "        X_, y_ = self.split_sequence(data, seq_length)\n",
    "        X_ = X_.reshape(X_.shape[0], num_channels, seq_length, *frame_size)\n",
    "        self.X = torch.from_numpy(\n",
    "            X_\n",
    "        ).float()  # numpy array data is double but the parameters in the nn layers are float.\n",
    "        self.y = torch.from_numpy(y_).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    # Probabably a better way here: https://diegslva.github.io/2017-05-02-first-post/\n",
    "    def split_sequence(self, data, nSeq):\n",
    "        X, y = list(), list()\n",
    "        nRow = data.shape[0]\n",
    "        for i in range(nRow):\n",
    "            # find the end of this pattern\n",
    "            end_ix = i + nSeq\n",
    "            # check if we are beyond the sequence\n",
    "            if end_ix > nRow - 1:\n",
    "                break\n",
    "                # gather input and output parts of the pattern\n",
    "            seq_x, seq_y = data[i:end_ix, :], data[end_ix, :]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Select sample\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MyDataset(dataScaled, seq_length, num_channels, frame_size)\n",
    "train_loader = DataLoader(ds, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_, phi_ = next(iter(train_loader))\n",
    "print(train_.shape)\n",
    "print(phi_.shape)\n",
    "train = train_[0, 0, -1, 0, :].cpu().detach().numpy()\n",
    "plt.plot(train)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(\n",
    "    num_channels, num_kernels, kernel_size, padding, activation, frame_size, num_layers\n",
    ")\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss(reduction=\"sum\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, patience=7, verbose=False, delta=0, path=\"checkpoint.pt\", trace_func=print\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(\n",
    "                f\"EarlyStopping counter: {self.counter} out of {self.patience}\"\n",
    "            )\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            self.trace_func(\n",
    "                f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\"\n",
    "            )\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.epoch = []\n",
    "        self.counter = 1\n",
    "    def __call__(self, train_loss, val_loss):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.epoch.append(self.counter)\n",
    "        self.counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "patience = 20\n",
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "    train_loss = 0\n",
    "    model.train()  # https://stackoverflow.com/a/51433411\n",
    "    for batch_num, (input, target) in enumerate(train_loader, 1):\n",
    "        output = model(input)\n",
    "        loss = criterion(output.flatten(), target.flatten())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    val_loss = 0\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        for input, target in train_loader:\n",
    "            output = model(input)\n",
    "            loss = criterion(output.flatten(), target.flatten())\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(train_loader.dataset)\n",
    "\n",
    "    history(train_loss, val_loss)\n",
    "    print(\n",
    "        \"Epoch:{} Training Loss:{:.8f} Validation Loss:{:.8f}\\n\".format(\n",
    "            epoch, train_loss, val_loss\n",
    "        )\n",
    "    )\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.epoch, history.train_losses)\n",
    "ax.plot(history.epoch, history.val_losses)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_, phi_ = next(iter(train_loader))\n",
    "model.eval()\n",
    "pred_ = model(train_)\n",
    "print(pred_.size())\n",
    "print(phi_.size())\n",
    "\n",
    "pred = pred_[-1, 0, 0, :].cpu().detach().numpy()\n",
    "phi = phi_[-1, :].cpu().detach().numpy()\n",
    "plt.plot(phi, \"--*\")\n",
    "plt.plot(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Seq2Seq(\n",
    "#     num_channels, num_kernels, kernel_size, padding, activation, frame_size, 10\n",
    "# )\n",
    "\n",
    "# optim = Adam(model.parameters(), lr=1e-4)\n",
    "# criterion = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "# input, target = next(iter((train_loader)))\n",
    "# output = model(input)\n",
    "# loss = criterion(output.flatten(), target.flatten())\n",
    "# loss.backward()\n",
    "# optim.step()\n",
    "# optim.zero_grad()\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single batch testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 1000\n",
    "# (input, target) = next(iter(train_loader))\n",
    "\n",
    "# for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "#     train_loss = 0\n",
    "#     model.train() #https://stackoverflow.com/a/51433411\n",
    "#     output = model(input)\n",
    "#     loss = criterion(output.flatten(), target.flatten())\n",
    "#     loss.backward()\n",
    "#     optim.step()\n",
    "#     optim.zero_grad()\n",
    "#     train_loss += loss.item()\n",
    "#     train_loss /= batch_size\n",
    "\n",
    "#     val_loss = 0\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         output = model(input)\n",
    "#         loss = criterion(output.flatten(), target.flatten())\n",
    "#         val_loss += loss.item()\n",
    "#     val_loss /= batch_size\n",
    "\n",
    "#     print(\n",
    "#         \"Epoch:{} Training Loss:{:.8f} Validation Loss:{:.8f}\\n\".format(\n",
    "#             epoch, train_loss, val_loss\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "\n",
    "# phiPred = output[-1,0,0,:].cpu().detach().numpy()\n",
    "# phi = target[-1,:].cpu().detach().numpy()\n",
    "# plt.plot(phi, '--*')\n",
    "# plt.plot(phiPred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d201a29f7c4182ebe0d045b868be9ef5a41c2ba4cccd19f1728f184a837bb8e6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
